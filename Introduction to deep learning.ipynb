{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN7K86YrkDCw8u9/9E9taEr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fALuIFcrU03c"},"outputs":[],"source":["#1.Explain what deep learning is and discuss its significance in the broader field of artificial intelligence."]},{"cell_type":"code","source":["Deep learning is a subset of machine learning, which itself is a branch of artificial intelligence (AI). It focuses on algorithms inspired by the structure and functioning of the human brain, known as artificial neural networks.\n","Deep learning models are distinguished by their depth ‚Äî they consist of multiple layers of neurons (or nodes) stacked together. Each layer processes and transforms data before passing it to the next, allowing the system to learn increasingly abstract representations.\n","\n","Deep learning involves the use of large datasets and high computational power, often leveraging Graphics Processing Units (GPUs) or specialized hardware like Tensor Processing Units (TPUs). Training a deep learning model typically involves optimization algorithms, such as stochastic gradient descent, to adjust the weights and biases of the network to minimize prediction errors.\n","\n","Key Components of Deep Learning\n","Artificial Neural Networks (ANNs): Composed of layers (input, hidden, and output), these are the building blocks of deep learning models.\n","Activation Functions: Non-linear functions (e.g., ReLU, Sigmoid, Tanh) that help the network learn complex patterns.\n","Loss Functions: Measure how well the model's predictions match the actual outcomes, guiding the optimization process.\n","Backpropagation: An algorithm for updating weights based on the loss gradient.\n","Significance in Artificial Intelligence\n","Deep learning has been transformative in advancing the broader field of AI for several reasons:\n","\n","1. State-of-the-Art Performance\n","Deep learning models achieve unparalleled accuracy and performance in tasks like image recognition, natural language processing, speech recognition, and more. Examples include:\n","\n","ImageNet classification challenges, dominated by convolutional neural networks (CNNs).\n","Language models like OpenAI's GPT and Google's BERT, excelling in text generation and comprehension.\n","2. End-to-End Learning\n","Unlike traditional machine learning, which often requires domain-specific feature engineering, deep learning can automatically extract and learn features from raw data, making it more flexible and scalable.\n","\n","3. Handling Complex Data Types\n","Deep learning models can process diverse data formats, including text, images, video, and audio, making them versatile for real-world applications.\n","\n","4. Enabling Key AI Applications\n","Deep learning underpins breakthroughs in various AI domains:\n","\n","Computer Vision: Used in autonomous vehicles, medical image analysis, and facial recognition.\n","Natural Language Processing (NLP): Powers chatbots, virtual assistants, and translation tools.\n","Generative Models: Produces new content, including text (e.g., ChatGPT), images, and videos.\n","5. Scalability\n","With sufficient computational resources and data, deep learning systems scale effectively, enabling continuous improvements.\n","\n","6. Reinforcement Learning Synergy\n","Deep learning combined with reinforcement learning has been pivotal in creating AI systems that excel in strategic tasks, such as AlphaGo mastering complex games.\n","\n","Challenges and Considerations\n","Despite its transformative impact, deep learning has limitations:\n","\n","Data Dependency: Requires large, labeled datasets.\n","Computational Costs: Training deep models can be resource-intensive.\n","Interpretability: Neural networks are often seen as \"black boxes,\" making it hard to understand their decision-making processes.\n","Ethical Concerns: Potential biases in data can lead to unintended consequences.\n","Conclusion\n","Deep learning represents a significant leap in the evolution of artificial intelligence, enabling systems to perform tasks that were once considered uniquely human. Its ability to model complex, non-linear relationships in data has revolutionized industries ranging from healthcare to entertainment, making it a cornerstone of modern AI research and applications."],"metadata":{"id":"kOMrMzCXVZFe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2. List and explain the fundamental components of artificial neural networks. 3.Discuss the roles of neurons, connections, weights, and biases"],"metadata":{"id":"MHVcf5fMVgCe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Artificial Neural Networks (ANNs) are computational models inspired by the structure of biological neural networks. The fundamental components of ANNs include:\n","\n","Input Layer\n","\n","The first layer of the network, where raw data is fed into the model. Each node in this layer represents one feature or attribute of the data.\n","Hidden Layers\n","\n","Intermediate layers between the input and output layers. They process and transform the data by performing computations and passing the results to subsequent layers. The number of hidden layers and nodes defines the depth and capacity of the network.\n","Output Layer\n","\n","The final layer that produces the output of the network, such as classifications, predictions, or decisions, depending on the problem being solved.\n","Neurons (Nodes)\n","\n","Fundamental units of the network that receive inputs, apply a transformation (typically using a weighted sum and activation function), and pass the result to the next layer.\n","Connections\n","\n","Links between neurons that transfer information. These connections have associated weights, determining the importance of the input from one neuron to another.\n","Weights\n","\n","Parameters that represent the strength of the connection between neurons. Weights are adjusted during training to minimize the error and improve the model's accuracy.\n","Biases\n","\n","Additional parameters added to the weighted sum input of a neuron. Biases allow the network to shift activation functions, improving its ability to model complex data.\n","Activation Functions\n","\n","Mathematical functions applied to the weighted sum of inputs to introduce non-linearity. Common activation functions include ReLU, Sigmoid, and Tanh.\n","Loss Function\n","\n","A function that measures the difference between the predicted output and the actual target. The goal of training is to minimize this loss.\n","Optimizer\n","\n","An algorithm (e.g., stochastic gradient descent) that updates weights and biases to minimize the loss function during training.\n","Forward Propagation\n","\n","The process of passing inputs through the network to produce an output.\n","Backpropagation\n","\n","A method for updating weights and biases by calculating the gradient of the loss function with respect to each parameter.\n","3. Roles of Neurons, Connections, Weights, and Biases\n","Neurons\n","\n","Act as the computational units of the network. Each neuron takes inputs, computes a weighted sum, applies an activation function, and produces an output.\n","They enable the network to process data and extract patterns.\n","Connections\n","\n","Represent pathways through which information flows between neurons.\n"," The structure and organization of these connections determine the complexity and capacity of the network.\n","Weights\n","\n","Define the significance of the input signals received by a neuron. Large weights amplify the input's contribution,\n"," while smaller weights diminish it. During training, weights are adjusted to improve the network's predictions.\n","Biases\n","\n","Allow neurons to activate independently of the input values by shifting the activation function.\n"," This enhances the network's ability to fit and generalize patterns in the data"],"metadata":{"id":"QOOBUqtEVf_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#4.Illustrate the architecture of an artificial neural network. Provide an example to explain the flow of information through the network.\n"],"metadata":{"id":"fhbu1FlyVf8R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["An ANN consists of an input layer, one or more hidden layers, and an output layer, with connections between the nodes in each layer. Below is a basic illustration of a feedforward neural network\n","\n","Input Layer:         Hidden Layer(s):            Output Layer:\n","(X1, X2, X3)    ---> (H1, H2, H3)        --->      (Y1, Y2)\n","Input Layer: Receives the raw features (e.g., X1, X2, X3).\n","Hidden Layers: Process data through neurons (e.g., H1, H2, H3) with weights, biases, and activation functions.\n","Output Layer: Produces the result (e.g., Y1, Y2).\n","Example: Predicting House Prices\n","Problem\n","We want to predict the price of a house based on three features:\n","\n","X1: Number of bedrooms.\n","X2: Size of the house (in square feet).\n","X3: Age of the house.\n","Architecture\n","Input Layer:\n","\n","Each input feature (X1, X2, X3) corresponds to a node in the input layer.\n","Hidden Layer:\n","\n","Contains three neurons (H1, H2, H3). Each neuron processes the inputs using weights, biases, and activation functions.\n"],"metadata":{"id":"ILkvB9IcVf5o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#5.Outline the perceptron learning algorithm. Describe how weights are adjusted during the learning process.\n"],"metadata":{"id":"A5-ZdU6UVfxT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Perceptron Learning Algorithm\n","The perceptron is one of the simplest types of artificial neural networks, used primarily for binary classification tasks. It works by finding a hyperplane that separates data points into two classes.\n","\n","Steps of the Perceptron Learning Algorithm\n","Initialization:\n","\n","Set initial weights (\n","ùë§1,ùë§2,‚Ä¶,ùë§ùëõw 1,w 2,‚Ä¶,wn\n","‚Äã) to small random values (or zeros).\n","Initialize the bias (ùëèb) to a small value.\n","Input the Data:\n","\n","Present one data sample\n","(ùë•,ùë¶)(x,y),\n","where ùë•x is the feature vector and ùë¶y is the target label (+1+1 or 1‚àí1).\n","Compute the Output:\n","\n","Calculate the weighted sum (net input):\n","ùëß=ùë§‚ãÖùë•+ùëè\n","z=w‚ãÖx+b\n","Apply the activation function (usually a step function) to determine the predicted output (\n","ùë¶^y^\n","‚Äã ):\n","ùë¶^={+1\n","if\n","ùëß‚â•0‚àí1\n","if\n","ùëß<0y^\n","‚Äã={ +1‚àí1\n","‚Äã\n","  if¬†z‚â•0\n","if¬†z<0\n","‚Äã\n","\n","\n","Here,\n","ùúÇ\n","Œ∑ is the learning rate (a small positive constant).\n","Repeat for All Data Points:\n","\n","Continue steps 2‚Äì4 for all training samples in the dataset.\n","Convergence:\n","\n","The algorithm stops when the perceptron correctly classifies all training data or after a predefined number of iterations.\n","How Weights Are Adjusted During Learning\n","The weight adjustment process is critical for the perceptron to learn a decision boundary. When the perceptron makes a mistake (i.e.,\n","\n"," )=‚àí2, and the weights are decreased.\n","Magnitude of Adjustment:\n","\n","The learning rate (\n","ùúÇ\n","Œ∑) controls how much the weights are updated in each step. A smaller\n","ùúÇ\n","Œ∑ leads to smaller, incremental updates, while a larger\n","ùúÇ\n","Œ∑ may lead to faster convergence but risks overshooting.\n","Bias Adjustment:\n","\n","The bias\n","ùëè\n","b is adjusted similarly to shift the decision boundary when needed.\n"],"metadata":{"id":"0gwuqREiWuu7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#6.Discuss the importance of activation functions in the hidden layers of a multi-layer perceptron. Provide  examples of commonly used activation functions"],"metadata":{"id":"fBMLKiurWusU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Importance of Activation Functions in the Hidden Layers of a Multi-Layer Perceptron (MLP)\n","Activation functions are critical components of artificial neural networks, especially in the hidden layers, as they introduce non-linearity to the network. Without activation functions, the entire network would behave as a linear model regardless of its depth, severely limiting its ability to model complex relationships in data.\n","\n","Key Roles of Activation Functions\n","Introducing Non-Linearity\n","\n","Real-world problems often involve non-linear relationships between input and output variables. Activation functions enable neural networks to learn and represent these non-linear mappings, which is essential for solving tasks like image recognition, language processing, and more.\n","Enabling Deep Learning\n","\n","By applying non-linear transformations, activation functions allow the stacking of multiple layers in a neural network to extract hierarchical and abstract features from data.\n","Controlling the Output Range\n","\n","Activation functions constrain the output of neurons to specific ranges, making training more stable and efficient. For instance, sigmoid functions map outputs to\n","[\n","0\n",",\n","1\n","]\n","[0,1], which is useful for probabilities.\n","Gradient Propagation\n","\n","Activation functions influence the gradients during backpropagation. Functions like ReLU mitigate the vanishing gradient problem, ensuring that gradients are large enough for efficient training.\n","Commonly Used Activation Functions\n","Here are the most widely used activation functions in neural networks:\n","\n","1. Sigmoid Function\n","ùëì\n","(\n","ùë•\n",")\n","=\n","1\n","1\n","+\n","ùëí\n","‚àí\n","ùë•\n","f(x)=\n","1+e\n","‚àíx\n","\n","1\n","‚Äã\n","\n","Output Range:\n","[\n","0\n",",\n","1\n","]\n","[0,1]\n","Characteristics:\n","Squashes input into a small range, making it suitable for probability-based tasks.\n","Limitations: Prone to the vanishing gradient problem, especially in deep networks.\n","2. Tanh (Hyperbolic Tangent) Function\n","ùëì\n","(\n","ùë•\n",")\n","=\n","ùëí\n","ùë•\n","‚àí\n","ùëí\n","‚àí\n","ùë•\n","ùëí\n","ùë•\n","+\n","ùëí\n","‚àí\n","ùë•\n","f(x)=\n","e\n","x\n"," +e\n","‚àíx\n","\n","e\n","x\n"," ‚àíe\n","‚àíx\n","\n","‚Äã\n","\n","Output Range:\n","[\n","‚àí\n","1\n",",\n","1\n","]\n","[‚àí1,1]\n","Characteristics:\n","Centered around 0, often preferred over sigmoid for hidden layers.\n","Limitations: Also suffers from the vanishing gradient problem.\n","3. ReLU (Rectified Linear Unit)\n","ùëì\n","(\n","ùë•\n",")\n","=\n","max\n","‚Å°\n","(\n","0\n",",\n","ùë•\n",")\n","f(x)=max(0,x)\n","Output Range:\n","[\n","0\n",",\n","‚àû\n",")\n","[0,‚àû)\n","Characteristics:\n","Simple and computationally efficient.\n","Helps mitigate the vanishing gradient problem.\n","Limitations: Can suffer from the dying ReLU problem, where neurons output 0 for all inputs.\n","4. Leaky ReLU\n","ùëì\n","(\n","ùë•\n",")\n","=\n","{\n","ùë•\n","if\n","ùë•\n",">\n","0\n","ùõº\n","ùë•\n","if\n","ùë•\n","‚â§\n","0\n","f(x)={\n","x\n","Œ±x\n","‚Äã\n","\n","if¬†x>0\n","if¬†x‚â§0\n","‚Äã\n","\n","Output Range:\n","(\n","‚àí\n","‚àû\n",",\n","‚àû\n",")\n","(‚àí‚àû,‚àû)\n","Characteristics:\n","Allows a small gradient (\n","ùõº\n",">\n","0\n","Œ±>0) for negative inputs, solving the dying ReLU problem.\n","5. Softmax Function\n","ùëì\n","(\n","ùë•\n","ùëñ\n",")\n","=\n","ùëí\n","ùë•\n","ùëñ\n","‚àë\n","ùëó\n","ùëí\n","ùë•\n","ùëó\n","f(x\n","i\n","‚Äã\n"," )=\n","‚àë\n","j\n","‚Äã\n"," e\n","x\n","j\n","‚Äã\n","\n","\n","e\n","x\n","i\n","‚Äã\n","\n","\n","‚Äã\n","\n","Output Range:\n","[\n","0\n",",\n","1\n","]\n","[0,1] (for each class)\n","Characteristics:\n","Converts outputs into probabilities, typically used in the output layer for multi-class classification.\n","6. ELU (Exponential Linear Unit)\n","ùëì\n","(\n","ùë•\n",")\n","=\n","{\n","ùë•\n","if\n","ùë•\n",">\n","0\n","ùõº\n","(\n","ùëí\n","ùë•\n","‚àí\n","1\n",")\n","if\n","ùë•\n","‚â§\n","0\n","f(x)={\n","x\n","Œ±(e\n","x\n"," ‚àí1)\n","‚Äã\n","\n","if¬†x>0\n","if¬†x‚â§0\n","‚Äã\n","\n","Output Range:\n","(\n","‚àí\n","ùõº\n",",\n","‚àû\n",")\n","(‚àíŒ±,‚àû)\n","Characteristics:\n","Similar to ReLU but smoother for negative inputs, reducing the risk of dying neurons.\n","Choosing the Right Activation Function\n","Hidden Layers:\n","\n","ReLU is the default choice due to its simplicity and effectiveness.\n","Tanh is used when the data is centered, and zero-mean outputs are preferred.\n","Leaky ReLU or ELU are alternatives to ReLU for avoiding dying neurons.\n","Output Layer:\n","\n","Sigmoid: For binary classification.\n","Softmax: For multi-class classification.\n","Example of Activation Functions in a Neural Network\n","Input: Raw data features (e.g., images, text embeddings).\n","Hidden Layer 1:\n","Apply ReLU to extract features.\n","Hidden Layer 2:\n","Use Tanh to refine the extracted features.\n","Output Layer:\n","Use Softmax to output probabilities for classification tasks.\n","By incorporating these activation functions, the network can learn intricate patterns and make accurate predictions."],"metadata":{"id":"oZyKvOykWupc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#1. Describe the basic structure of a Feedforward Neural Network (FNN). What is the purpose of the activation function"],"metadata":{"id":"zioOCVo-YojD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Basic Structure of a Feedforward Neural Network (FNN)\n","A Feedforward Neural Network (FNN) is the simplest type of artificial neural network, where connections between nodes do not form cycles. The information flows in one direction‚Äîfrom the input layer, through the hidden layers, to the output layer.\n","\n","Components of an FNN:\n","Input Layer:\n","\n","Receives raw data. Each node corresponds to a feature of the input data.\n","Hidden Layers:\n","\n","One or more intermediate layers where neurons apply transformations to extract patterns and features.\n","Each neuron computes a weighted sum of inputs, adds a bias, and applies an activation function.\n","Output Layer:\n","\n","Produces the network's output, which could be a classification, regression value, or other prediction based on the problem type.\n","Connections:\n","\n","Links between neurons carry information. Each connection has a weight that adjusts during training.\n","Purpose of the Activation Function (In Short)\n","The activation function introduces non-linearity into the network, allowing it to model complex relationships between input and output. Without activation functions, the network would behave as a linear model, regardless of its depth. Key benefits include:\n","\n","Enabling the network to learn intricate patterns.\n","Allowing stacking of layers to extract hierarchical features.\n","Transforming inputs into ranges suitable for specific tasks, like probabilities (\n"],"metadata":{"id":"_nbDJBB6Yoff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2 Explain the role of convolutional layers in CNN. Why are pooling layers commonly used, and what do they achieve"],"metadata":{"id":"zEH1PL4XYocj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Role of Convolutional Layers in CNNs\n","Convolutional layers are the core building blocks of Convolutional Neural Networks (CNNs). Their main roles are:\n","\n","Feature Extraction:\n","\n","Convolutional layers extract spatial and hierarchical features from input data (like images) using convolution operations. Filters (or kernels) slide over the input, detecting patterns such as edges, textures, and objects.\n","Parameter Sharing:\n","\n","Filters are shared across the input, significantly reducing the number of parameters compared to fully connected layers, making the network more efficient.\n","Preserve Spatial Structure:\n","\n","Convolutional layers retain the spatial relationships between pixels, crucial for tasks like image recognition or object detection.\n","Receptive Field:\n","\n","Each neuron in the convolutional layer focuses on a small region (local receptive field) of the input, enabling the network to learn localized patterns.\n","Why Are Pooling Layers Commonly Used?\n","Pooling layers are commonly used in CNNs to downsample feature maps and achieve the following:\n","\n","Dimensionality Reduction:\n","\n","Reduce the spatial dimensions of feature maps, which lowers computational cost and prevents overfitting.\n","Feature Invariance:\n","\n","Help the model focus on the presence of features rather than their exact positions, making the network more robust to translations and distortions.\n","Highlighting Dominant Features:\n","\n","Pooling layers, especially max pooling, retain only the most prominent features within a region, simplifying feature representation.\n","What Do Pooling Layers Achieve (In Short)?\n","Pooling layers achieve dimensionality reduction, improve computational efficiency, and enhance feature invariance to spatial transformations like translation or rotation. This makes CNNs more effective for tasks like image and video processing.\n"],"metadata":{"id":"NR9B7jJoYoZf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3 What is the key characteristic that differentiates Recurrent Neural Networks (RNNs) from other neural networks? How does an RNN handle sequential data"],"metadata":{"id":"MYxdC_rpYoWt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Key Characteristic of RNNs\n","The key characteristic that differentiates Recurrent Neural Networks (RNNs) from other neural networks is their ability to process sequential data by maintaining a memory of past inputs. RNNs achieve this through recurrent connections, where the output of a neuron at a given time step is fed back as an input for the next time step.\n","\n","How RNNs Handle Sequential Data (In Short)\n","RNNs handle sequential data by:\n","\n","Temporal Dependencies:\n","\n","RNNs take inputs one step at a time and maintain a hidden state that acts as a memory, capturing information from previous time steps.\n","Recurrent Connections:\n","\n","Output Generation:\n","\n","The hidden state is used to produce outputs sequentially, enabling the network to model time-series data, natural language, or other sequences.\n","This architecture allows RNNs to learn patterns in sequential data, such as trends, dependencies, or contextual relationships."],"metadata":{"id":"fx4DoaRoZVs_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#4 . Discuss the components of a Long Short-Term Memory (LSTM) network. How does it address the vanishing gradient problem"],"metadata":{"id":"htf8zz8lZVjJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Components of a Long Short-Term Memory (LSTM) Network\n","An LSTM network is a specialized type of RNN designed to handle long-term dependencies in sequential data. Its architecture includes unique components to control the flow of information:\n","\n","Cell State (\n","ùê∂ùë°C t):\n","\n","Acts as a memory, carrying information across time steps. It allows the network to retain or discard information selectively.\n","\n","Output Gate:\n","\n","How LSTMs Address the Vanishing Gradient Problem (In Short)\n","LSTMs mitigate the vanishing gradient problem through their gated architecture:\n","\n","Forget Gate:\n","\n","Allows the network to selectively retain important information over long time periods.\n","Cell State:\n","\n","The cell state provides a direct, unbroken path for gradients to flow, reducing their decay during backpropagation.\n","Non-linear Activation Functions in Gates:\n","\n","The use of sigmoid and\n","tanh functions ensures controlled updates and prevents drastic changes to gradients.\n","These mechanisms enable LSTMs to learn dependencies over long sequences,\n","making them effective for tasks like language modeling, speech recognition, and time-series prediction."],"metadata":{"id":"IX75nUwZZVfy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#5 Describe the roles of the generator and discriminator in a Generative Adversarial Network (GAN). What is the training objective for each"],"metadata":{"id":"ERz_Eee4ZVcH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Roles of the Generator and Discriminator in a GAN\n","A Generative Adversarial Network (GAN) consists of two components that compete with and improve each other through an adversarial process:\n","\n","Generator:\n","\n","Role: Generates fake data samples (e.g., images, text) that resemble real data.\n","Goal: Fool the discriminator into classifying fake samples as real.\n","Discriminator:\n","\n","Role: Distinguishes between real data samples (from the dataset) and fake samples (generated by the generator).\n","Goal: Accurately classify real and fake samples.\n","Training Objectives\n","Generator‚Äôs Objective:\n","\n","Maximize the discriminator‚Äôs probability of classifying fake data as real:\n","max\n","\n"," E[log(D(G(z)))]\n","Here,\n","ùëß\n","z is the noise input to the generator\n","D(‚ãÖ) is the discriminator‚Äôs probability output.\n","Discriminator‚Äôs Objective:\n","\n","Minimize its classification error for both real and fake data:\n","\n"," E[log(D(x))]+E[log(1‚àíD(G(z)))]\n","G(z) is a generated (fake) sample.\n","In Short\n","The generator aims to produce realistic samples to deceive the discriminator.\n","The discriminator works to distinguish real from fake samples.\n","Together, they compete in a min-max optimization, improving the quality of generated data over time."],"metadata":{"id":"DkLiwXKuZVYl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"O1D8fxAfZVVK"},"execution_count":null,"outputs":[]}]}